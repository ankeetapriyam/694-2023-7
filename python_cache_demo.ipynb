{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecd5086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thefuzz\n",
      "  Downloading thefuzz-0.19.0-py2.py3-none-any.whl (17 kB)\n",
      "Installing collected packages: thefuzz\n",
      "Successfully installed thefuzz-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19ce309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bd28fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "def fetch_cache():\n",
    "    cached_data=[]\n",
    "    if(os.path.isfile(\"CacheFile.json\")):\n",
    "        with open(\"CacheFile.json\",\"r\") as cache_file:\n",
    "            cached_data = json.load(cache_file)['cached_queries']\n",
    "    return cached_data\n",
    "\n",
    "def get_top_cache_data(cached_data):\n",
    "    cached_queries=[]\n",
    "    potential_cached_queries=[]\n",
    "    if(bool(cached_data)):\n",
    "        counter=0\n",
    "        for key in cached_data:\n",
    "            cached_queries.append(key) if counter<5 else potential_cached_queries.append(key)\n",
    "            counter+=1\n",
    "    return cached_queries,potential_cached_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dbb5e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search_Cache(search_string):\n",
    "    cached_data = fetch_cache()\n",
    "    search_result=[]\n",
    "    if(bool(cached_data)):\n",
    "        cached_queries, potential_cached_queries = get_top_cache_data(cached_data)\n",
    "        if(bool(cached_queries)):\n",
    "            match_query = process.extractOne(search_string, cached_queries , scorer = fuzz.token_set_ratio)\n",
    "            if(match_query[1]>50):\n",
    "                cached_data[match_query[0]]['counter']+=1\n",
    "                search_result = cached_data[match_query[0]]['result']\n",
    "                #Calling refresh\n",
    "                cached_data = refresh(cached_data)\n",
    "                update_cache(cached_data)\n",
    "                return search_result\n",
    "            elif(potential_cached_queries):\n",
    "                match_query = process.extractOne(search_string, potential_cached_queries)\n",
    "                if(match_query[1]>50):\n",
    "                    cached_data[match_query[0]]['counter']+=1\n",
    "                    #Calling refresh\n",
    "                    search_result=[]\n",
    "                    cached_data = refresh(cached_data)\n",
    "                    update_cache(cached_data)\n",
    "                    return search_result\n",
    "        return search_result;\n",
    "\n",
    "def Write_Cache(search_string,search_result):\n",
    "    cached_data = fetch_cache()\n",
    "    if(len(cached_data)<20):\n",
    "        cached_data[search_string] = {\n",
    "            \"counter\":1,\n",
    "            \"result\":search_result\n",
    "        }\n",
    "        cached_data = refresh(cached_data)\n",
    "        update_cache(cached_data)\n",
    "            \n",
    "# Refresh the dictionary with sorted count\n",
    "def refresh(cached_data):\n",
    "    dict_cache_small = {}\n",
    "    len_small = len(cached_data)\n",
    "    dict_big = dict(sorted(cached_data.items() , key = lambda x : x[1]['counter'], reverse = True))\n",
    "    keys = list(dict_big.keys())\n",
    "    for i in range(0,len_small ):\n",
    "        \n",
    "        dict_cache_small[keys[i]] = dict_big[keys[i]]\n",
    "    return dict_cache_small\n",
    "\n",
    "\n",
    "# Update the cache file on RAM with latest score\n",
    "def update_cache(cached_data):\n",
    "    cached_data_write={\"cached_queries\":cached_data}\n",
    "    json_string = json.dumps(cached_data_write)\n",
    "    with open(\"CacheFile.json\",\"w\") as cache_file:\n",
    "        cache_file.write(json_string);\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1f3c7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_string = \"top billboard artists\"\n",
    "result = [\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:27:32\",\n",
    "                    \"id_str\": \"1249403795643043840\",\n",
    "                    \"text\": \"This Monday I\\u2019m Dj\\u2019ing a vinyl set on Diggers Factory to gather money for Corona Virus  research. A lot of artists\\u2026 https://t.co/9D5A5841sS\",\n",
    "                    \"retweet_count\": 0,\n",
    "                    \"user_id_str\": \"2983288664\",\n",
    "                    \"user_name\": \"John Tejada\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 5.943684101104736\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:27:43\",\n",
    "                    \"id_str\": \"1249403843009368064\",\n",
    "                    \"text\": \"RT @himdaughter: Be worried, be very worried.\\nCorona not a disease for the underprivileged anywhere in the world.\\nCalifornia Tops Two Milli\\u2026\",\n",
    "                    \"retweet_count\": 0,\n",
    "                    \"user_id_str\": \"90804011\",\n",
    "                    \"user_name\": \"Dr Soumitra Pathare\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 5.823451995849609\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:08:42\",\n",
    "                    \"id_str\": \"1249399056272707591\",\n",
    "                    \"text\": \"Be worried, be very worried.\\nCorona not a disease for the underprivileged anywhere in the world.\\nCalifornia Tops Tw\\u2026 https://t.co/sDmvkqDG2o\",\n",
    "                    \"retweet_count\": 1,\n",
    "                    \"user_id_str\": \"551910958\",\n",
    "                    \"user_name\": \"Shailja\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 5.823451995849609\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:28:08\",\n",
    "                    \"id_str\": \"1249403945660715010\",\n",
    "                    \"text\": \"RT @mollycrabapple: This little boy looks like he\\u2019s ten tops, and these big officers, some with no masks, are traumatizing him\\n\\nDe Blasio s\\u2026\",\n",
    "                    \"retweet_count\": 0,\n",
    "                    \"user_id_str\": \"56831437\",\n",
    "                    \"user_name\": \"Brandon Davis\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 5.750097274780273\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 14:48:48\",\n",
    "                    \"id_str\": \"1249348749043867653\",\n",
    "                    \"text\": \"This little boy looks like he\\u2019s ten tops, and these big officers, some with no masks, are traumatizing him\\n\\nDe Blas\\u2026 https://t.co/69bQYvMxDM\",\n",
    "                    \"retweet_count\": 209,\n",
    "                    \"user_id_str\": \"15644999\",\n",
    "                    \"user_name\": \"Molly Crabapple\\ud83c\\uddf5\\ud83c\\uddf7\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 5.750097274780273\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:29:31\",\n",
    "                    \"id_str\": \"1249404294291390464\",\n",
    "                    \"text\": \"RT @Mckbenna: The fact that they were already in such a severe lockdown, we can't even compare. And having Corona on top of that and then T\\u2026\",\n",
    "                    \"retweet_count\": 0,\n",
    "                    \"user_id_str\": \"740121769646907393\",\n",
    "                    \"user_name\": \"hadiya butt\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 5.671999931335449\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:27:05\",\n",
    "                    \"id_str\": \"1249403682262863872\",\n",
    "                    \"text\": \"The fact that they were already in such a severe lockdown, we can't even compare. And having Corona on top of that\\u2026 https://t.co/PkYOhuvwEw\",\n",
    "                    \"retweet_count\": 1,\n",
    "                    \"user_id_str\": \"1085761150313213952\",\n",
    "                    \"user_name\": \"Ashi\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 5.671999931335449\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:27:33\",\n",
    "                    \"id_str\": \"1249403799984394250\",\n",
    "                    \"text\": \"RT @hemirdesai: How will u do shooting pro #CoronaVirus when 50 or less will be allowed. U can cast @taapsee n secular artists as they thin\\u2026\",\n",
    "                    \"retweet_count\": 0,\n",
    "                    \"user_id_str\": \"1156634363393708033\",\n",
    "                    \"user_name\": \"#Dev Oza | \\u0926\\u0947\\u0935 |\",\n",
    "                    \"hashtags\": [\n",
    "                        \"CoronaVirus\"\n",
    "                    ],\n",
    "                    \"score\": 5.53331184387207\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-11 13:40:32\",\n",
    "                    \"id_str\": \"1248969182026190849\",\n",
    "                    \"text\": \"How will u do shooting pro #CoronaVirus when 50 or less will be allowed. U can cast @taapsee n secular artists as t\\u2026 https://t.co/lwAwhyNO4P\",\n",
    "                    \"retweet_count\": 45,\n",
    "                    \"user_id_str\": \"165641975\",\n",
    "                    \"user_name\": \"Hemir Desai\",\n",
    "                    \"hashtags\": [\n",
    "                        \"CoronaVirus\"\n",
    "                    ],\n",
    "                    \"score\": 5.408830165863037\n",
    "                },\n",
    "                {\n",
    "                    \"created_at\": \"2020-04-12 18:28:11\",\n",
    "                    \"id_str\": \"1249403960315858948\",\n",
    "                    \"text\": \"Then please tell me again the UK is not racist.\",\n",
    "                    \"retweet_count\": 0,\n",
    "                    \"user_id_str\": \"1686687122\",\n",
    "                    \"user_name\": \"Beatrice \\ud83c\\udf39\",\n",
    "                    \"hashtags\": [],\n",
    "                    \"score\": 2.2354636192321777\n",
    "                }\n",
    "            ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a4d8eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_string = \"Joseph for president\"\n",
    "#search_string =\"global warming\"\n",
    "#search_string =\"caSes on the rise\"\n",
    "search_string = \"billboard artists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3ccb5a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'billboard artists'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "55e4c670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created_at': '2020-04-12 18:27:32', 'id_str': '1249403795643043840', 'text': 'This Monday I’m Dj’ing a vinyl set on Diggers Factory to gather money for Corona Virus  research. A lot of artists… https://t.co/9D5A5841sS', 'retweet_count': 0, 'user_id_str': '2983288664', 'user_name': 'John Tejada', 'hashtags': [], 'score': 5.943684101104736}, {'created_at': '2020-04-12 18:27:43', 'id_str': '1249403843009368064', 'text': 'RT @himdaughter: Be worried, be very worried.\\nCorona not a disease for the underprivileged anywhere in the world.\\nCalifornia Tops Two Milli…', 'retweet_count': 0, 'user_id_str': '90804011', 'user_name': 'Dr Soumitra Pathare', 'hashtags': [], 'score': 5.823451995849609}, {'created_at': '2020-04-12 18:08:42', 'id_str': '1249399056272707591', 'text': 'Be worried, be very worried.\\nCorona not a disease for the underprivileged anywhere in the world.\\nCalifornia Tops Tw… https://t.co/sDmvkqDG2o', 'retweet_count': 1, 'user_id_str': '551910958', 'user_name': 'Shailja', 'hashtags': [], 'score': 5.823451995849609}, {'created_at': '2020-04-12 18:28:08', 'id_str': '1249403945660715010', 'text': 'RT @mollycrabapple: This little boy looks like he’s ten tops, and these big officers, some with no masks, are traumatizing him\\n\\nDe Blasio s…', 'retweet_count': 0, 'user_id_str': '56831437', 'user_name': 'Brandon Davis', 'hashtags': [], 'score': 5.750097274780273}, {'created_at': '2020-04-12 14:48:48', 'id_str': '1249348749043867653', 'text': 'This little boy looks like he’s ten tops, and these big officers, some with no masks, are traumatizing him\\n\\nDe Blas… https://t.co/69bQYvMxDM', 'retweet_count': 209, 'user_id_str': '15644999', 'user_name': 'Molly Crabapple🇵🇷', 'hashtags': [], 'score': 5.750097274780273}, {'created_at': '2020-04-12 18:29:31', 'id_str': '1249404294291390464', 'text': \"RT @Mckbenna: The fact that they were already in such a severe lockdown, we can't even compare. And having Corona on top of that and then T…\", 'retweet_count': 0, 'user_id_str': '740121769646907393', 'user_name': 'hadiya butt', 'hashtags': [], 'score': 5.671999931335449}, {'created_at': '2020-04-12 18:27:05', 'id_str': '1249403682262863872', 'text': \"The fact that they were already in such a severe lockdown, we can't even compare. And having Corona on top of that… https://t.co/PkYOhuvwEw\", 'retweet_count': 1, 'user_id_str': '1085761150313213952', 'user_name': 'Ashi', 'hashtags': [], 'score': 5.671999931335449}, {'created_at': '2020-04-12 18:27:33', 'id_str': '1249403799984394250', 'text': 'RT @hemirdesai: How will u do shooting pro #CoronaVirus when 50 or less will be allowed. U can cast @taapsee n secular artists as they thin…', 'retweet_count': 0, 'user_id_str': '1156634363393708033', 'user_name': '#Dev Oza | देव |', 'hashtags': ['CoronaVirus'], 'score': 5.53331184387207}, {'created_at': '2020-04-11 13:40:32', 'id_str': '1248969182026190849', 'text': 'How will u do shooting pro #CoronaVirus when 50 or less will be allowed. U can cast @taapsee n secular artists as t… https://t.co/lwAwhyNO4P', 'retweet_count': 45, 'user_id_str': '165641975', 'user_name': 'Hemir Desai', 'hashtags': ['CoronaVirus'], 'score': 5.408830165863037}, {'created_at': '2020-04-12 18:28:11', 'id_str': '1249403960315858948', 'text': 'Then please tell me again the UK is not racist.', 'retweet_count': 0, 'user_id_str': '1686687122', 'user_name': 'Beatrice 🌹', 'hashtags': [], 'score': 2.2354636192321777}]\n"
     ]
    }
   ],
   "source": [
    "search_result=Search_Cache(search_string)\n",
    "print(search_result)\n",
    "\n",
    "if(search_result==None):\n",
    "    Write_Cache(search_string,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aeb2edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c17bc632",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"mongodb+srv://grmongodb:Mongodb321@clustertwitter0.qx1igmo.mongodb.net/?retryWrites=true&w=majority\"\n",
    "db_name = \"twitterdatabase\"\n",
    "collection_name = \"collection001\"\n",
    "\n",
    "cluster = MongoClient(url)\n",
    "db = cluster[db_name]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ff400959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'collection001'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "465a7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9168b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "db710cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7785609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fc4f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is america\n"
     ]
    }
   ],
   "source": [
    "s = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccaab97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is america\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "92b52ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokenize(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \" \", text)  # Remove punctuation\n",
    "    \n",
    "    text = contractions.fix(text)\n",
    "    text = \" \".join(text.split())  # Remove extra spaces, tabs, and new lines\n",
    "    tokens =  word_tokenize(text)\n",
    "    return [token for token in tokens if token not in stopwords and token not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6bb19452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brain', 'working']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tokenize(\"your brain's not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "818c3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = preprocess_tokenize(\"this is america-19 this //\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc17a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl2 = ['this', 'is', 'america', '19', 'this' , 'donald', 'glover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c402caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_match(l1, l2):\n",
    "    nm = set(l1).intersection(l2)\n",
    "    return len(nm)/len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98cdf7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_main_dict = {\n",
    "    \n",
    "    \"'this', 'is', 'america', '19', 'this' , 'donald', 'glover'\" : {'score' : 2 , 'out' : ''} \n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "598fd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[\"'this', 'is', 'america', '19', 'this' , 'donald', 'glover'\"]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303511f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 fill the dictionary\n",
    "# step 2 make a function to increment and retrieve \n",
    "# step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a85aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"cache_text_input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0099f7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "skip\n",
      "skip\n",
      "skip\n",
      "skip\n",
      "skip\n",
      "skip\n",
      "skip\n",
      "skip\n",
      "skip\n"
     ]
    }
   ],
   "source": [
    "in_list  = []\n",
    "file_read = open(file, 'r', encoding = 'utf-8')\n",
    "lines = file_read.readlines()\n",
    "sent_tokenize(lines[0])\n",
    "for l in lines:\n",
    "    if len(l) <= 2:\n",
    "        print(\"skip\")\n",
    "        continue\n",
    "    sentences = sent_tokenize(l)\n",
    "    for sen in sentences:\n",
    "        in_list.append(preprocess_tokenize(str (sen)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4856583e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['know', 'eat', 'standup', 'gig', 'tv', 'show'],\n",
       " ['brain', '’', 'working', 'fast', 'eat'],\n",
       " ['slower', 'tired'],\n",
       " ['similar', 'feel', 'post', 'coital', 'right'],\n",
       " ['postprandial',\n",
       "  'post',\n",
       "  'coital',\n",
       "  'similar',\n",
       "  'far',\n",
       "  'body',\n",
       "  'concerned',\n",
       "  'mission',\n",
       "  'accomplished',\n",
       "  'today'],\n",
       " ['nothing',\n",
       "  'else',\n",
       "  'needs',\n",
       "  'achieved',\n",
       "  'eaten',\n",
       "  'procreated',\n",
       "  '–',\n",
       "  'smashing'],\n",
       " ['go', 'bed', 'earned'],\n",
       " ['course', 'comedian', 'working', 'day', 'starts', 'body', 'know'],\n",
       " ['real', 'high', 'low', 'person'],\n",
       " ['love', 'michelin', 'star', 'tasting', 'menu'],\n",
       " ['love', 'zinger', 'meal'],\n",
       " ['elitist', 'comes', 'food'],\n",
       " ['love'],\n",
       " ['hit',\n",
       "  'nine',\n",
       "  '10',\n",
       "  'years',\n",
       "  'old',\n",
       "  'became',\n",
       "  '…',\n",
       "  '“',\n",
       "  'voracious',\n",
       "  '”',\n",
       "  'would',\n",
       "  'word'],\n",
       " ['malaysia',\n",
       "  'would',\n",
       "  'go',\n",
       "  'coffee',\n",
       "  'shops',\n",
       "  'would',\n",
       "  'full',\n",
       "  'dish',\n",
       "  'noodles',\n",
       "  'starter',\n",
       "  'full',\n",
       "  'plate',\n",
       "  'grilled',\n",
       "  'meats',\n",
       "  'second',\n",
       "  'course'],\n",
       " ['double', 'meals'],\n",
       " ['got', 'fat', 'age', '10', 'maybe', 'even', '15', 'big', 'boy'],\n",
       " ['people', 'talk', 'human', 'eye', 'proof', 'intelligent', 'design'],\n",
       " ['prawn'],\n",
       " ['lot',\n",
       "  'wrong',\n",
       "  'eye',\n",
       "  'eye',\n",
       "  'fucks',\n",
       "  'time',\n",
       "  'blind',\n",
       "  'spot',\n",
       "  'wearing',\n",
       "  'glasses'],\n",
       " ['eye', 'sucks'],\n",
       " ['eye', 'proof', 'god'],\n",
       " ['prawn', 'perfect', 'curve', 'sea', 'meat'],\n",
       " ['eat'],\n",
       " ['animal', 'take', 'skin', 'put', 'whole', 'thing', 'mouth'],\n",
       " ['like', 'designed', 'us', 'eat'],\n",
       " ['trip',\n",
       "  'china',\n",
       "  '2019',\n",
       "  'food',\n",
       "  'market',\n",
       "  'one',\n",
       "  'stall',\n",
       "  'guy',\n",
       "  'selling',\n",
       "  'critters',\n",
       "  'sticks'],\n",
       " ['snakes', 'iguanas', 'lizard', 'splayed', 'like', 'crucified'],\n",
       " ['big', 'old', 'tarantula', 'stick', 'ate'],\n",
       " ['ate', 'centipede', 'stick'],\n",
       " ['went', 'cockroach', 'farm', 'ate', 'cockroaches'],\n",
       " ['would', 'eat', 'eat', 'baked', 'salmon'],\n",
       " ['really', 'would'],\n",
       " ['imagine', 'vegetarian'],\n",
       " ['vegan'],\n",
       " ['could', 'live', 'without', 'eggs'],\n",
       " ['egg', 'like', 'prawn', 'comes', 'perfect', 'little', 'package'],\n",
       " ['edible'],\n",
       " ['good'],\n",
       " ['sweet', 'savoury'],\n",
       " ['else', 'omelette', 'whiskey', 'sour'],\n",
       " ['nothing'],\n",
       " ['miracle', 'food'],\n",
       " ['british', 'approach', 'food', 'alcohol', 'boom', 'bust'],\n",
       " ['like',\n",
       "  'drink',\n",
       "  'anything',\n",
       "  'week',\n",
       "  'weekend',\n",
       "  'drink',\n",
       "  'much',\n",
       "  'potentially',\n",
       "  'commit',\n",
       "  'crime'],\n",
       " ['week',\n",
       "  'eat',\n",
       "  'plain',\n",
       "  'gentle',\n",
       "  'food',\n",
       "  'friday',\n",
       "  'eat',\n",
       "  'vindaloo',\n",
       "  'ruin',\n",
       "  'night'],\n",
       " ['get',\n",
       "  'absolutely',\n",
       "  'crucified',\n",
       "  'still',\n",
       "  'entirely',\n",
       "  'get',\n",
       "  'yorkshire',\n",
       "  'puddings'],\n",
       " ['bread', 'bowl', 'shape'],\n",
       " ['bread', 'gravy', 'cup'],\n",
       " ['give',\n",
       "  'british',\n",
       "  'food',\n",
       "  'tough',\n",
       "  'time',\n",
       "  'bland',\n",
       "  'make',\n",
       "  'exception',\n",
       "  'puddings',\n",
       "  'country',\n",
       "  'well'],\n",
       " ['banoffee', 'pie', 'top', 'list'],\n",
       " ['salty',\n",
       "  'sweetness',\n",
       "  'toffee',\n",
       "  'airiness',\n",
       "  'cream',\n",
       "  'comforting',\n",
       "  'banana',\n",
       "  'taste',\n",
       "  'without',\n",
       "  'whole',\n",
       "  'banana',\n",
       "  'jammed',\n",
       "  'throat',\n",
       "  'maybe',\n",
       "  'know',\n",
       "  'eat',\n",
       "  'bananas'],\n",
       " ['got',\n",
       "  'ideal',\n",
       "  'taste',\n",
       "  'profile',\n",
       "  'british',\n",
       "  'puddings',\n",
       "  'without',\n",
       "  'heaviness',\n",
       "  'say',\n",
       "  'sticky',\n",
       "  'toffee',\n",
       "  'pudding',\n",
       "  'also',\n",
       "  'great',\n",
       "  'way'],\n",
       " ['phil', 'wang'],\n",
       " ['phil', 'wang', '‘', 'would', 'like', 'remembered'],\n",
       " ['man', 'solved', 'nuclear', 'fusion', '’'],\n",
       " ['read'],\n",
       " ['got',\n",
       "  'wine',\n",
       "  'late',\n",
       "  '20s',\n",
       "  'controversial',\n",
       "  'wine',\n",
       "  'opinion',\n",
       "  'think',\n",
       "  'wine',\n",
       "  'gets',\n",
       "  'better',\n",
       "  'past',\n",
       "  '£30'],\n",
       " ['could',\n",
       "  'draw',\n",
       "  'quality',\n",
       "  'versus',\n",
       "  'price',\n",
       "  'line',\n",
       "  'would',\n",
       "  'start',\n",
       "  'plateauing',\n",
       "  'around',\n",
       "  '30',\n",
       "  'certain',\n",
       "  'point',\n",
       "  'really',\n",
       "  'paying',\n",
       "  'quality'],\n",
       " ['paying', 'prestige', 'rarity'],\n",
       " ['lot', 'comedians', 'food'],\n",
       " ['might',\n",
       "  'lot',\n",
       "  'free',\n",
       "  'time',\n",
       "  'travel',\n",
       "  'get',\n",
       "  'food',\n",
       "  'way',\n",
       "  'people',\n",
       "  'nine',\n",
       "  'fives',\n",
       "  'probably',\n",
       "  'less',\n",
       "  'time'],\n",
       " ['hedonists', 'really'],\n",
       " ['work',\n",
       "  'based',\n",
       "  'pleasure',\n",
       "  'really',\n",
       "  'appreciate',\n",
       "  'pleasure',\n",
       "  'weare',\n",
       "  'always',\n",
       "  'chasing',\n",
       "  'pleasure']]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c4157170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cache = {}\n",
    "for item in in_list:\n",
    "    dict_cache[str(item)] = { 'score' : 1 , 'output' : item }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1680b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_str = \"pie is on top of my list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4da502a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_retrieve(s):\n",
    "    tokens = preprocess_tokenize(s)\n",
    "\n",
    "    for key in list(dict_cache.keys()):\n",
    "        if perc_match( tokens, eval(key)) > 0.4:\n",
    "            dict_cache[key]['score'] = dict_cache[key]['score'] + 1\n",
    "            print(dict_cache[key]['output'] ,dict_cache[key]['score'] )\n",
    "    cache_main = refresh()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fb428891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['banoffee', 'pie', 'top', 'list']\",\n",
       "  {'score': 6, 'output': ['banoffee', 'pie', 'top', 'list']}),\n",
       " (\"['love', 'michelin', 'star', 'tasting', 'menu']\",\n",
       "  {'score': 3, 'output': ['love', 'michelin', 'star', 'tasting', 'menu']}),\n",
       " (\"['love']\", {'score': 3, 'output': ['love']}),\n",
       " (\"['miracle', 'food']\", {'score': 3, 'output': ['miracle', 'food']}),\n",
       " (\"['similar', 'feel', 'post', 'coital', 'right']\",\n",
       "  {'score': 2, 'output': ['similar', 'feel', 'post', 'coital', 'right']}),\n",
       " (\"['love', 'zinger', 'meal']\",\n",
       "  {'score': 2, 'output': ['love', 'zinger', 'meal']}),\n",
       " (\"['elitist', 'comes', 'food']\",\n",
       "  {'score': 2, 'output': ['elitist', 'comes', 'food']}),\n",
       " (\"['lot', 'comedians', 'food']\",\n",
       "  {'score': 2, 'output': ['lot', 'comedians', 'food']}),\n",
       " (\"['know', 'eat', 'standup', 'gig', 'tv', 'show']\",\n",
       "  {'score': 1, 'output': ['know', 'eat', 'standup', 'gig', 'tv', 'show']}),\n",
       " (\"['brain', '’', 'working', 'fast', 'eat']\",\n",
       "  {'score': 1, 'output': ['brain', '’', 'working', 'fast', 'eat']}),\n",
       " (\"['slower', 'tired']\", {'score': 1, 'output': ['slower', 'tired']}),\n",
       " (\"['postprandial', 'post', 'coital', 'similar', 'far', 'body', 'concerned', 'mission', 'accomplished', 'today']\",\n",
       "  {'score': 1,\n",
       "   'output': ['postprandial',\n",
       "    'post',\n",
       "    'coital',\n",
       "    'similar',\n",
       "    'far',\n",
       "    'body',\n",
       "    'concerned',\n",
       "    'mission',\n",
       "    'accomplished',\n",
       "    'today']}),\n",
       " (\"['nothing', 'else', 'needs', 'achieved', 'eaten', 'procreated', '–', 'smashing']\",\n",
       "  {'score': 1,\n",
       "   'output': ['nothing',\n",
       "    'else',\n",
       "    'needs',\n",
       "    'achieved',\n",
       "    'eaten',\n",
       "    'procreated',\n",
       "    '–',\n",
       "    'smashing']}),\n",
       " (\"['go', 'bed', 'earned']\", {'score': 1, 'output': ['go', 'bed', 'earned']}),\n",
       " (\"['course', 'comedian', 'working', 'day', 'starts', 'body', 'know']\",\n",
       "  {'score': 1,\n",
       "   'output': ['course',\n",
       "    'comedian',\n",
       "    'working',\n",
       "    'day',\n",
       "    'starts',\n",
       "    'body',\n",
       "    'know']}),\n",
       " (\"['real', 'high', 'low', 'person']\",\n",
       "  {'score': 1, 'output': ['real', 'high', 'low', 'person']}),\n",
       " (\"['hit', 'nine', '10', 'years', 'old', 'became', '…', '“', 'voracious', '”', 'would', 'word']\",\n",
       "  {'score': 1,\n",
       "   'output': ['hit',\n",
       "    'nine',\n",
       "    '10',\n",
       "    'years',\n",
       "    'old',\n",
       "    'became',\n",
       "    '…',\n",
       "    '“',\n",
       "    'voracious',\n",
       "    '”',\n",
       "    'would',\n",
       "    'word']}),\n",
       " (\"['malaysia', 'would', 'go', 'coffee', 'shops', 'would', 'full', 'dish', 'noodles', 'starter', 'full', 'plate', 'grilled', 'meats', 'second', 'course']\",\n",
       "  {'score': 1,\n",
       "   'output': ['malaysia',\n",
       "    'would',\n",
       "    'go',\n",
       "    'coffee',\n",
       "    'shops',\n",
       "    'would',\n",
       "    'full',\n",
       "    'dish',\n",
       "    'noodles',\n",
       "    'starter',\n",
       "    'full',\n",
       "    'plate',\n",
       "    'grilled',\n",
       "    'meats',\n",
       "    'second',\n",
       "    'course']}),\n",
       " (\"['double', 'meals']\", {'score': 1, 'output': ['double', 'meals']}),\n",
       " (\"['got', 'fat', 'age', '10', 'maybe', 'even', '15', 'big', 'boy']\",\n",
       "  {'score': 1,\n",
       "   'output': ['got',\n",
       "    'fat',\n",
       "    'age',\n",
       "    '10',\n",
       "    'maybe',\n",
       "    'even',\n",
       "    '15',\n",
       "    'big',\n",
       "    'boy']}),\n",
       " (\"['people', 'talk', 'human', 'eye', 'proof', 'intelligent', 'design']\",\n",
       "  {'score': 1,\n",
       "   'output': ['people',\n",
       "    'talk',\n",
       "    'human',\n",
       "    'eye',\n",
       "    'proof',\n",
       "    'intelligent',\n",
       "    'design']}),\n",
       " (\"['prawn']\", {'score': 1, 'output': ['prawn']}),\n",
       " (\"['lot', 'wrong', 'eye', 'eye', 'fucks', 'time', 'blind', 'spot', 'wearing', 'glasses']\",\n",
       "  {'score': 1,\n",
       "   'output': ['lot',\n",
       "    'wrong',\n",
       "    'eye',\n",
       "    'eye',\n",
       "    'fucks',\n",
       "    'time',\n",
       "    'blind',\n",
       "    'spot',\n",
       "    'wearing',\n",
       "    'glasses']}),\n",
       " (\"['eye', 'sucks']\", {'score': 1, 'output': ['eye', 'sucks']}),\n",
       " (\"['eye', 'proof', 'god']\", {'score': 1, 'output': ['eye', 'proof', 'god']}),\n",
       " (\"['prawn', 'perfect', 'curve', 'sea', 'meat']\",\n",
       "  {'score': 1, 'output': ['prawn', 'perfect', 'curve', 'sea', 'meat']}),\n",
       " (\"['eat']\", {'score': 1, 'output': ['eat']}),\n",
       " (\"['animal', 'take', 'skin', 'put', 'whole', 'thing', 'mouth']\",\n",
       "  {'score': 1,\n",
       "   'output': ['animal', 'take', 'skin', 'put', 'whole', 'thing', 'mouth']}),\n",
       " (\"['like', 'designed', 'us', 'eat']\",\n",
       "  {'score': 1, 'output': ['like', 'designed', 'us', 'eat']}),\n",
       " (\"['trip', 'china', '2019', 'food', 'market', 'one', 'stall', 'guy', 'selling', 'critters', 'sticks']\",\n",
       "  {'score': 1,\n",
       "   'output': ['trip',\n",
       "    'china',\n",
       "    '2019',\n",
       "    'food',\n",
       "    'market',\n",
       "    'one',\n",
       "    'stall',\n",
       "    'guy',\n",
       "    'selling',\n",
       "    'critters',\n",
       "    'sticks']}),\n",
       " (\"['snakes', 'iguanas', 'lizard', 'splayed', 'like', 'crucified']\",\n",
       "  {'score': 1,\n",
       "   'output': ['snakes', 'iguanas', 'lizard', 'splayed', 'like', 'crucified']}),\n",
       " (\"['big', 'old', 'tarantula', 'stick', 'ate']\",\n",
       "  {'score': 1, 'output': ['big', 'old', 'tarantula', 'stick', 'ate']}),\n",
       " (\"['ate', 'centipede', 'stick']\",\n",
       "  {'score': 1, 'output': ['ate', 'centipede', 'stick']}),\n",
       " (\"['went', 'cockroach', 'farm', 'ate', 'cockroaches']\",\n",
       "  {'score': 1, 'output': ['went', 'cockroach', 'farm', 'ate', 'cockroaches']}),\n",
       " (\"['would', 'eat', 'eat', 'baked', 'salmon']\",\n",
       "  {'score': 1, 'output': ['would', 'eat', 'eat', 'baked', 'salmon']}),\n",
       " (\"['really', 'would']\", {'score': 1, 'output': ['really', 'would']}),\n",
       " (\"['imagine', 'vegetarian']\",\n",
       "  {'score': 1, 'output': ['imagine', 'vegetarian']}),\n",
       " (\"['vegan']\", {'score': 1, 'output': ['vegan']}),\n",
       " (\"['could', 'live', 'without', 'eggs']\",\n",
       "  {'score': 1, 'output': ['could', 'live', 'without', 'eggs']}),\n",
       " (\"['egg', 'like', 'prawn', 'comes', 'perfect', 'little', 'package']\",\n",
       "  {'score': 1,\n",
       "   'output': ['egg',\n",
       "    'like',\n",
       "    'prawn',\n",
       "    'comes',\n",
       "    'perfect',\n",
       "    'little',\n",
       "    'package']}),\n",
       " (\"['edible']\", {'score': 1, 'output': ['edible']}),\n",
       " (\"['good']\", {'score': 1, 'output': ['good']}),\n",
       " (\"['sweet', 'savoury']\", {'score': 1, 'output': ['sweet', 'savoury']}),\n",
       " (\"['else', 'omelette', 'whiskey', 'sour']\",\n",
       "  {'score': 1, 'output': ['else', 'omelette', 'whiskey', 'sour']}),\n",
       " (\"['nothing']\", {'score': 1, 'output': ['nothing']}),\n",
       " (\"['british', 'approach', 'food', 'alcohol', 'boom', 'bust']\",\n",
       "  {'score': 1,\n",
       "   'output': ['british', 'approach', 'food', 'alcohol', 'boom', 'bust']}),\n",
       " (\"['like', 'drink', 'anything', 'week', 'weekend', 'drink', 'much', 'potentially', 'commit', 'crime']\",\n",
       "  {'score': 1,\n",
       "   'output': ['like',\n",
       "    'drink',\n",
       "    'anything',\n",
       "    'week',\n",
       "    'weekend',\n",
       "    'drink',\n",
       "    'much',\n",
       "    'potentially',\n",
       "    'commit',\n",
       "    'crime']}),\n",
       " (\"['week', 'eat', 'plain', 'gentle', 'food', 'friday', 'eat', 'vindaloo', 'ruin', 'night']\",\n",
       "  {'score': 1,\n",
       "   'output': ['week',\n",
       "    'eat',\n",
       "    'plain',\n",
       "    'gentle',\n",
       "    'food',\n",
       "    'friday',\n",
       "    'eat',\n",
       "    'vindaloo',\n",
       "    'ruin',\n",
       "    'night']}),\n",
       " (\"['get', 'absolutely', 'crucified', 'still', 'entirely', 'get', 'yorkshire', 'puddings']\",\n",
       "  {'score': 1,\n",
       "   'output': ['get',\n",
       "    'absolutely',\n",
       "    'crucified',\n",
       "    'still',\n",
       "    'entirely',\n",
       "    'get',\n",
       "    'yorkshire',\n",
       "    'puddings']}),\n",
       " (\"['bread', 'bowl', 'shape']\",\n",
       "  {'score': 1, 'output': ['bread', 'bowl', 'shape']}),\n",
       " (\"['bread', 'gravy', 'cup']\",\n",
       "  {'score': 1, 'output': ['bread', 'gravy', 'cup']}),\n",
       " (\"['give', 'british', 'food', 'tough', 'time', 'bland', 'make', 'exception', 'puddings', 'country', 'well']\",\n",
       "  {'score': 1,\n",
       "   'output': ['give',\n",
       "    'british',\n",
       "    'food',\n",
       "    'tough',\n",
       "    'time',\n",
       "    'bland',\n",
       "    'make',\n",
       "    'exception',\n",
       "    'puddings',\n",
       "    'country',\n",
       "    'well']}),\n",
       " (\"['salty', 'sweetness', 'toffee', 'airiness', 'cream', 'comforting', 'banana', 'taste', 'without', 'whole', 'banana', 'jammed', 'throat', 'maybe', 'know', 'eat', 'bananas']\",\n",
       "  {'score': 1,\n",
       "   'output': ['salty',\n",
       "    'sweetness',\n",
       "    'toffee',\n",
       "    'airiness',\n",
       "    'cream',\n",
       "    'comforting',\n",
       "    'banana',\n",
       "    'taste',\n",
       "    'without',\n",
       "    'whole',\n",
       "    'banana',\n",
       "    'jammed',\n",
       "    'throat',\n",
       "    'maybe',\n",
       "    'know',\n",
       "    'eat',\n",
       "    'bananas']}),\n",
       " (\"['got', 'ideal', 'taste', 'profile', 'british', 'puddings', 'without', 'heaviness', 'say', 'sticky', 'toffee', 'pudding', 'also', 'great', 'way']\",\n",
       "  {'score': 1,\n",
       "   'output': ['got',\n",
       "    'ideal',\n",
       "    'taste',\n",
       "    'profile',\n",
       "    'british',\n",
       "    'puddings',\n",
       "    'without',\n",
       "    'heaviness',\n",
       "    'say',\n",
       "    'sticky',\n",
       "    'toffee',\n",
       "    'pudding',\n",
       "    'also',\n",
       "    'great',\n",
       "    'way']}),\n",
       " (\"['phil', 'wang']\", {'score': 1, 'output': ['phil', 'wang']}),\n",
       " (\"['phil', 'wang', '‘', 'would', 'like', 'remembered']\",\n",
       "  {'score': 1,\n",
       "   'output': ['phil', 'wang', '‘', 'would', 'like', 'remembered']}),\n",
       " (\"['man', 'solved', 'nuclear', 'fusion', '’']\",\n",
       "  {'score': 1, 'output': ['man', 'solved', 'nuclear', 'fusion', '’']}),\n",
       " (\"['read']\", {'score': 1, 'output': ['read']}),\n",
       " (\"['got', 'wine', 'late', '20s', 'controversial', 'wine', 'opinion', 'think', 'wine', 'gets', 'better', 'past', '£30']\",\n",
       "  {'score': 1,\n",
       "   'output': ['got',\n",
       "    'wine',\n",
       "    'late',\n",
       "    '20s',\n",
       "    'controversial',\n",
       "    'wine',\n",
       "    'opinion',\n",
       "    'think',\n",
       "    'wine',\n",
       "    'gets',\n",
       "    'better',\n",
       "    'past',\n",
       "    '£30']}),\n",
       " (\"['could', 'draw', 'quality', 'versus', 'price', 'line', 'would', 'start', 'plateauing', 'around', '30', 'certain', 'point', 'really', 'paying', 'quality']\",\n",
       "  {'score': 1,\n",
       "   'output': ['could',\n",
       "    'draw',\n",
       "    'quality',\n",
       "    'versus',\n",
       "    'price',\n",
       "    'line',\n",
       "    'would',\n",
       "    'start',\n",
       "    'plateauing',\n",
       "    'around',\n",
       "    '30',\n",
       "    'certain',\n",
       "    'point',\n",
       "    'really',\n",
       "    'paying',\n",
       "    'quality']}),\n",
       " (\"['paying', 'prestige', 'rarity']\",\n",
       "  {'score': 1, 'output': ['paying', 'prestige', 'rarity']}),\n",
       " (\"['might', 'lot', 'free', 'time', 'travel', 'get', 'food', 'way', 'people', 'nine', 'fives', 'probably', 'less', 'time']\",\n",
       "  {'score': 1,\n",
       "   'output': ['might',\n",
       "    'lot',\n",
       "    'free',\n",
       "    'time',\n",
       "    'travel',\n",
       "    'get',\n",
       "    'food',\n",
       "    'way',\n",
       "    'people',\n",
       "    'nine',\n",
       "    'fives',\n",
       "    'probably',\n",
       "    'less',\n",
       "    'time']}),\n",
       " (\"['hedonists', 'really']\", {'score': 1, 'output': ['hedonists', 'really']}),\n",
       " (\"['work', 'based', 'pleasure', 'really', 'appreciate', 'pleasure', 'weare', 'always', 'chasing', 'pleasure']\",\n",
       "  {'score': 1,\n",
       "   'output': ['work',\n",
       "    'based',\n",
       "    'pleasure',\n",
       "    'really',\n",
       "    'appreciate',\n",
       "    'pleasure',\n",
       "    'weare',\n",
       "    'always',\n",
       "    'chasing',\n",
       "    'pleasure']})]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dict_cache.items() , key = lambda x : x[1]['score'], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "97f56183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def refresh():\n",
    "    dict_cache_small = {}\n",
    "    len_small = 50\n",
    "    dict_big = dict(sorted(dict_cache.items() , key = lambda x : x[1]['score'], reverse = True))\n",
    "    keys = list(dict_big.keys())\n",
    "    for i in range(0,len_small ):\n",
    "        \n",
    "        dict_cache_small[keys[i]] = dict_big[keys[i]]\n",
    "    return dict_cache_small\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3fa476e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_main = refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "565ff9a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "british alcohol approach food like boom\n"
     ]
    }
   ],
   "source": [
    "search_string  = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b66ce9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['miracle', 'food'] 8\n",
      "['british', 'approach', 'food', 'alcohol', 'boom', 'bust'] 6\n"
     ]
    }
   ],
   "source": [
    "search_retrieve(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "69d51add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['banoffee', 'pie', 'top', 'list']\": {'score': 7,\n",
       "  'output': ['banoffee', 'pie', 'top', 'list']},\n",
       " \"['love', 'michelin', 'star', 'tasting', 'menu']\": {'score': 5,\n",
       "  'output': ['love', 'michelin', 'star', 'tasting', 'menu']},\n",
       " \"['love']\": {'score': 3, 'output': ['love']},\n",
       " \"['miracle', 'food']\": {'score': 8, 'output': ['miracle', 'food']},\n",
       " \"['similar', 'feel', 'post', 'coital', 'right']\": {'score': 2,\n",
       "  'output': ['similar', 'feel', 'post', 'coital', 'right']},\n",
       " \"['love', 'zinger', 'meal']\": {'score': 2,\n",
       "  'output': ['love', 'zinger', 'meal']},\n",
       " \"['elitist', 'comes', 'food']\": {'score': 2,\n",
       "  'output': ['elitist', 'comes', 'food']},\n",
       " \"['lot', 'comedians', 'food']\": {'score': 2,\n",
       "  'output': ['lot', 'comedians', 'food']},\n",
       " \"['know', 'eat', 'standup', 'gig', 'tv', 'show']\": {'score': 1,\n",
       "  'output': ['know', 'eat', 'standup', 'gig', 'tv', 'show']},\n",
       " \"['brain', '’', 'working', 'fast', 'eat']\": {'score': 1,\n",
       "  'output': ['brain', '’', 'working', 'fast', 'eat']},\n",
       " \"['slower', 'tired']\": {'score': 1, 'output': ['slower', 'tired']},\n",
       " \"['postprandial', 'post', 'coital', 'similar', 'far', 'body', 'concerned', 'mission', 'accomplished', 'today']\": {'score': 1,\n",
       "  'output': ['postprandial',\n",
       "   'post',\n",
       "   'coital',\n",
       "   'similar',\n",
       "   'far',\n",
       "   'body',\n",
       "   'concerned',\n",
       "   'mission',\n",
       "   'accomplished',\n",
       "   'today']},\n",
       " \"['nothing', 'else', 'needs', 'achieved', 'eaten', 'procreated', '–', 'smashing']\": {'score': 1,\n",
       "  'output': ['nothing',\n",
       "   'else',\n",
       "   'needs',\n",
       "   'achieved',\n",
       "   'eaten',\n",
       "   'procreated',\n",
       "   '–',\n",
       "   'smashing']},\n",
       " \"['go', 'bed', 'earned']\": {'score': 1, 'output': ['go', 'bed', 'earned']},\n",
       " \"['course', 'comedian', 'working', 'day', 'starts', 'body', 'know']\": {'score': 1,\n",
       "  'output': ['course',\n",
       "   'comedian',\n",
       "   'working',\n",
       "   'day',\n",
       "   'starts',\n",
       "   'body',\n",
       "   'know']},\n",
       " \"['real', 'high', 'low', 'person']\": {'score': 1,\n",
       "  'output': ['real', 'high', 'low', 'person']},\n",
       " \"['hit', 'nine', '10', 'years', 'old', 'became', '…', '“', 'voracious', '”', 'would', 'word']\": {'score': 1,\n",
       "  'output': ['hit',\n",
       "   'nine',\n",
       "   '10',\n",
       "   'years',\n",
       "   'old',\n",
       "   'became',\n",
       "   '…',\n",
       "   '“',\n",
       "   'voracious',\n",
       "   '”',\n",
       "   'would',\n",
       "   'word']},\n",
       " \"['malaysia', 'would', 'go', 'coffee', 'shops', 'would', 'full', 'dish', 'noodles', 'starter', 'full', 'plate', 'grilled', 'meats', 'second', 'course']\": {'score': 1,\n",
       "  'output': ['malaysia',\n",
       "   'would',\n",
       "   'go',\n",
       "   'coffee',\n",
       "   'shops',\n",
       "   'would',\n",
       "   'full',\n",
       "   'dish',\n",
       "   'noodles',\n",
       "   'starter',\n",
       "   'full',\n",
       "   'plate',\n",
       "   'grilled',\n",
       "   'meats',\n",
       "   'second',\n",
       "   'course']},\n",
       " \"['double', 'meals']\": {'score': 1, 'output': ['double', 'meals']},\n",
       " \"['got', 'fat', 'age', '10', 'maybe', 'even', '15', 'big', 'boy']\": {'score': 1,\n",
       "  'output': ['got', 'fat', 'age', '10', 'maybe', 'even', '15', 'big', 'boy']},\n",
       " \"['people', 'talk', 'human', 'eye', 'proof', 'intelligent', 'design']\": {'score': 1,\n",
       "  'output': ['people',\n",
       "   'talk',\n",
       "   'human',\n",
       "   'eye',\n",
       "   'proof',\n",
       "   'intelligent',\n",
       "   'design']},\n",
       " \"['prawn']\": {'score': 1, 'output': ['prawn']},\n",
       " \"['lot', 'wrong', 'eye', 'eye', 'fucks', 'time', 'blind', 'spot', 'wearing', 'glasses']\": {'score': 1,\n",
       "  'output': ['lot',\n",
       "   'wrong',\n",
       "   'eye',\n",
       "   'eye',\n",
       "   'fucks',\n",
       "   'time',\n",
       "   'blind',\n",
       "   'spot',\n",
       "   'wearing',\n",
       "   'glasses']},\n",
       " \"['eye', 'sucks']\": {'score': 1, 'output': ['eye', 'sucks']},\n",
       " \"['eye', 'proof', 'god']\": {'score': 1, 'output': ['eye', 'proof', 'god']},\n",
       " \"['prawn', 'perfect', 'curve', 'sea', 'meat']\": {'score': 1,\n",
       "  'output': ['prawn', 'perfect', 'curve', 'sea', 'meat']},\n",
       " \"['eat']\": {'score': 1, 'output': ['eat']},\n",
       " \"['animal', 'take', 'skin', 'put', 'whole', 'thing', 'mouth']\": {'score': 1,\n",
       "  'output': ['animal', 'take', 'skin', 'put', 'whole', 'thing', 'mouth']},\n",
       " \"['like', 'designed', 'us', 'eat']\": {'score': 1,\n",
       "  'output': ['like', 'designed', 'us', 'eat']},\n",
       " \"['trip', 'china', '2019', 'food', 'market', 'one', 'stall', 'guy', 'selling', 'critters', 'sticks']\": {'score': 1,\n",
       "  'output': ['trip',\n",
       "   'china',\n",
       "   '2019',\n",
       "   'food',\n",
       "   'market',\n",
       "   'one',\n",
       "   'stall',\n",
       "   'guy',\n",
       "   'selling',\n",
       "   'critters',\n",
       "   'sticks']},\n",
       " \"['snakes', 'iguanas', 'lizard', 'splayed', 'like', 'crucified']\": {'score': 1,\n",
       "  'output': ['snakes', 'iguanas', 'lizard', 'splayed', 'like', 'crucified']},\n",
       " \"['big', 'old', 'tarantula', 'stick', 'ate']\": {'score': 1,\n",
       "  'output': ['big', 'old', 'tarantula', 'stick', 'ate']},\n",
       " \"['ate', 'centipede', 'stick']\": {'score': 1,\n",
       "  'output': ['ate', 'centipede', 'stick']},\n",
       " \"['went', 'cockroach', 'farm', 'ate', 'cockroaches']\": {'score': 1,\n",
       "  'output': ['went', 'cockroach', 'farm', 'ate', 'cockroaches']},\n",
       " \"['would', 'eat', 'eat', 'baked', 'salmon']\": {'score': 1,\n",
       "  'output': ['would', 'eat', 'eat', 'baked', 'salmon']},\n",
       " \"['really', 'would']\": {'score': 1, 'output': ['really', 'would']},\n",
       " \"['imagine', 'vegetarian']\": {'score': 1,\n",
       "  'output': ['imagine', 'vegetarian']},\n",
       " \"['vegan']\": {'score': 1, 'output': ['vegan']},\n",
       " \"['could', 'live', 'without', 'eggs']\": {'score': 1,\n",
       "  'output': ['could', 'live', 'without', 'eggs']},\n",
       " \"['egg', 'like', 'prawn', 'comes', 'perfect', 'little', 'package']\": {'score': 1,\n",
       "  'output': ['egg', 'like', 'prawn', 'comes', 'perfect', 'little', 'package']},\n",
       " \"['edible']\": {'score': 1, 'output': ['edible']},\n",
       " \"['good']\": {'score': 1, 'output': ['good']},\n",
       " \"['sweet', 'savoury']\": {'score': 1, 'output': ['sweet', 'savoury']},\n",
       " \"['else', 'omelette', 'whiskey', 'sour']\": {'score': 1,\n",
       "  'output': ['else', 'omelette', 'whiskey', 'sour']},\n",
       " \"['nothing']\": {'score': 1, 'output': ['nothing']},\n",
       " \"['british', 'approach', 'food', 'alcohol', 'boom', 'bust']\": {'score': 6,\n",
       "  'output': ['british', 'approach', 'food', 'alcohol', 'boom', 'bust']},\n",
       " \"['like', 'drink', 'anything', 'week', 'weekend', 'drink', 'much', 'potentially', 'commit', 'crime']\": {'score': 1,\n",
       "  'output': ['like',\n",
       "   'drink',\n",
       "   'anything',\n",
       "   'week',\n",
       "   'weekend',\n",
       "   'drink',\n",
       "   'much',\n",
       "   'potentially',\n",
       "   'commit',\n",
       "   'crime']},\n",
       " \"['week', 'eat', 'plain', 'gentle', 'food', 'friday', 'eat', 'vindaloo', 'ruin', 'night']\": {'score': 1,\n",
       "  'output': ['week',\n",
       "   'eat',\n",
       "   'plain',\n",
       "   'gentle',\n",
       "   'food',\n",
       "   'friday',\n",
       "   'eat',\n",
       "   'vindaloo',\n",
       "   'ruin',\n",
       "   'night']},\n",
       " \"['get', 'absolutely', 'crucified', 'still', 'entirely', 'get', 'yorkshire', 'puddings']\": {'score': 1,\n",
       "  'output': ['get',\n",
       "   'absolutely',\n",
       "   'crucified',\n",
       "   'still',\n",
       "   'entirely',\n",
       "   'get',\n",
       "   'yorkshire',\n",
       "   'puddings']},\n",
       " \"['bread', 'bowl', 'shape']\": {'score': 1,\n",
       "  'output': ['bread', 'bowl', 'shape']}}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641911b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9a4ff99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_by_tweet(tweet_str = None):\n",
    "    try:\n",
    "        if tweet_str:\n",
    "            words = word_tokenize(tweet_str)\n",
    "            non_stop_words = sorted([word for word in words if not (word.lower() in stopwords or word in punctuation)])\n",
    "            query = {\n",
    "                \"$and\": [\n",
    "                     {'$text': {'$search': ' '.join(non_stop_words)}},\n",
    "                     {\"oc_tweet_id\": \"\"}\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            tweets = collection.find(query, {'created_at': 1, 'retweet_count': 1, 'user_id_str': 1, 'oc_tweet_id': 1, '_id': 0}).sort([('retweet_count', pymongo.DESCENDING)]).limit(10)\n",
    "            return tweets\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d3ccc18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = get_info_by_tweet(\"thankful for staff risking their life for maintaining the telecom service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "932792f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tweets:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
